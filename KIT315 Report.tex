\documentclass[runningheads]{llncs}
\usepackage{graphicx}

\begin{document}
%
\title{Penguin Object Detection Model Development}

\author{Xunyi Zhao}
\institute{University of Tasmania, Newnham TAS 7248, AU}
\authorrunning{X.}
%
\maketitle
%
\begin{abstract}
Penguin, a near threatened species lives in the southern hemisphere, has been influenced due to the global warming issue recently. The aims of this project is to develop an object detection model, which can  detect and classify the type of penguin from images/videos to help Antarctic scientific research. In this project, popular approaches for object detection such as YOLO, R-CNN are used to achieve good performance. On average, the AP50 can reach 87\%+ with the FPS of 150+ by using YOLOv7 model, which is 1.5x times faster and +6.3\% AP more accurate than YOLOv5 and even more than Faster R-CNN.

\keywords{Antarctica penguins \and Deep learning \and Object detection \and YOLO \and Faster R-CNN \and Model selection.}
\end{abstract}
%
%

% Section 1: Motivation/benefits
\section{Motivation/Benefits}
\subsection{Motivation}
Penguins, a group of aquatic flightless birds, live almost exclusively in the Southern Hemisphere in Antarctica. Among the 20 living species, the \textbf{\textit{Aptenodytes Forsteri (Emperor Penguin)}}, \textbf{\textit{Aptenodytes Patagonicus (King Penguin)}} and \textbf{\textit{Pygoscelis Antarciticus (Chinstrap Penguin)}} are three of the most common and popular species. 

However, due to the global warming and other issues, emperor penguins have been identified as Vulnerable or Near Threatened in the latest IUCN Red List~\cite{ref_red_list}. Sometimes, we may find one or two penguins are isolated on a floating ice (can not find continent or suitable are for living within 10 kilometres around it).

Thus, I have the motivation to help researchers detect this situation, and also help them to classify the species of penguins automatically, or potentially count the number of penguins in the specific area. Hence, I selected this project and wish to develop an appropriate machine learning application.
\subsection{Real-life benefits}
In real life scenario, this application can detect and then identify the species (only from emperor penguins, king penguins and chinstrap penguins due to the time restriction) of penguins live in Antarctica, and count the number of penguins potentially in real-time. What's more, if only one or two penguins are detected within a big area, the system that embedded with this model can send notification, which allows human noticing this situation quickly and providing corresponding aid if necessary. This will benefit both penguin (ensure population ecological balance) and human (needn't monitor manually) a lot.


% Section 2: Data collecting
\section{Data collecting}
In this project, nearly all of the data is collected from Google Image~\cite{ref_google_image}. To ensure the quality of the data and helps the model achieve good performance, I have filtered the bad images manually, for example, images with too many penguins, blurry images, low-resolution images etc.

What's more, since the purpose of the application is to detect and then identify the species (and count the number potentially), it is important for me to collect the images that contains different numbers of penguins (only 1 penguin or many penguins), clear to see (the higher resolution, the better), and in various context and scenario. Besides, the number of samples and penguin instances should also be large enough, and both penguin adults and chicks (they look pretty different) should be included in the data set. To ensure the quality of the data collecting process, if any situation in the following list happens, then I will not collect that image:

\begin{itemize}
  \item If the image size is too small.
  \item If the image ratio is too weird (say 16:3, which means it's not very square).
  \item If the image contains too many penguins (\textgreater{100}) or blurry.
  \item If the penguins is drawn by human.
\end{itemize}

If the image resolution (size) is low, the image will be even more blurry after resizing before/during the training process, so that the model could not extract the key features of the image and the performance will be impacted. Also, if the image is too narrow (strange image ratio), then the object will be squeezed/stretched too much after resizing, causing difficulties for model to recognized these objects. Besides, painting images should not be collected because they have many difference with real penguin images. All this aspects will be discussed and analysed again in the model selection part.

Another step in data collecting process is the data augmentation. Augmentation performs transforms on the existing images to create new variations and increase the number of images in the data set. This ultimately makes models more accurate across a broader range of use cases. 
In the real scenario, the environment we monitor may contain some noise and the exposure of the photo/video of the observed environment will be different, which would affect the average precision of the model. To help the model be more resilient to camera artifacts, lighting and camera setting changes, I choose to generate more pictures with different \textbf{noise} (Up to 3\% of pixels) and \textbf{exposure} (Between -15\% and +15\%) by using Roboflow tool~\cite{ref_roboflow}


% Section 3: Data annotating
\section{Data annotating}
To annotate the data that I've collected, I also used Roboflow~\cite{ref_roboflow}. Each types of penguins can be labeled using bounding box with different colors within a short time, and the annotation can be exported in various forms (COCO, JSON, TXT etc.) for training different models. The following are the rules I stick with to ensure the quality of the data annotating process:

\begin{itemize}
  \item Make sure the bounding box cover the whole body of each penguin.
  \item If the image contains many penguins, and some of the penguins are too small/blurry/not distinctly visible, just annotate the clear penguins that stand in the front. *
  \item If only the head of the penguin can be seen, make sure to annotate the head.
  \item Annotate the penguin chick as well.
\end{itemize}

* The reason why I ignored some penguins that are too small/blurry/not distinctly visible is that, although as human, we can know that these "objects" are penguins, but if the model learns too many of these "information", it would have some problems such as over-fitting or can not detect the penguins correctly, it will treat any similar blurry things as penguin which is not good for general case. However, it doesn't mean computer could not detect the blurry penguins since computer vision deals with images in pixel-level but human is not. If the application is used for counting the number of penguins in the image, then I should label all the penguins that I can see. But the main purpose is to detect and classify, to balance this situation and ensure the model has high generalizability, I choose to ignore the blurry penguins in the background.

% Section 4: Data processing
\section{Data processing}
Data processing is a crucial phase in the machine learning process since the quality of the data and the information that can be extracted from it directly influence how well the model can learn. For this reason, it is crucial that we pre-process the data before introducing it to the model.

A crucial step in computer vision's pre-processing is image resizing. Since the images I collected don't have the same size, although in some models like YOLOv5, the image resizing is done automatically. However, other models like R-CNN will not. Principally, deep learning models train faster on small images. A larger input image requires the neural network to learn from four times as many pixels, which increases the architecture's training time~\cite{resize}. Thus, I choose the resize option in Roboflow~\cite{ref_roboflow}.

Usually, an image is captured with metadata that specifies how it should be displayed in relation to how the pixels are arranged on disc. This directive, which is stored in the EXIF orientation field, expedites the image encoding process at the time of capture, allowing cameras to efficiently sample data from their sensors without unwelcome artefacts. This means that most cameras store images' pixels exactly the same whether the camera is oriented in landscape or portrait mode. They just flip a bit to signal to the viewer whether to display the pixels as-is or to rotate them by 90 or 180 degrees when displaying the image. Unfortunately, this can cause issues if the application displaying the images is unaware of the metadata and naively displays the image without respecting its EXIF orientation. Thus, to help the model learn better, I choose the auto-Orient option in Roboflow~\cite{ref_roboflow}.

During the data annotating, I used the scientific name for each type of penguin which is hard for general user to understand. Thus, to help user know what the species are when the model detecting objects, I choose to modify the class name (for example, King Penguin replaces Aptenodytes Patagonicus).

Since the data set does not contain missing values/annotations, I needn't perform any operations about it, and I didn't need to normalize the data as well (keep the original color as it is). 

After the data collecting and the data processing, there have 1188 images in the data set, and the train/validation/test split I use is 84\%/9\%/7\%. 


% Section 5: Model design
\section{Model selection/design}
After processing the data, I need to select and design appropriate approaches (three in total) for this project. 

Among different approaches for object detection tasks, the Region-based Convolutional Neural Network (R-CNN), as a famous two-step object, is suitable for this project. It separates the object detection and recognition into two phase, and can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment object. When there is a lack of labelled training data, supervised pre-training for a secondary task followed by domain-specific fine-tuning results in a noticeable performance improvement~\cite{R-CNN}. Although R-CNN is slower than the one-step object detection models, it can achieve better average precision potentially. Faster R-CNN, as it's name, is the fastest R-CNN. The `region proposal network' (RPN), a component that uses the feature maps generated by a convolutional neural network to suggest a set of bounding boxes where objects may be located, is the main innovation in this system. By incorporating region detection into the primary neural network architecture,Faster R-CNN is able to detect objects quickly and nearly in real time~\cite{Faster R-CNN}.

YOLO, as a one-step object detection model, is the integration of the entire object detection and classification process~\cite{YOLO}. Instead of extracting features and regions separately, YOLO performs everything in a single pass through a single network. In addition, YOLO can perform object detection at video streaming framerates and is suitable applications that require real-time inference like this project. YOLO has published 7 different versions by this year, and the most two popular versions are the YOLOv5 and the latest version YOLOv7. The reason why I don't use YOLOv6 is it defines the model parameters directly in Python instead of YAML file, which means it has less customizability than YOLOv5. YOLOv7, on the other hand, is published in August and is 120\% faster than the other versions. In order to compare the difference, I will choose both versions of YOLO. 

Hence, the final three models I choose for this project are YOLOv5, YOLOv7, and Faster R-CNN.

% Section 6: Model selection
\section{Model selection}
\subsection{Selection strategy and metrics used}

\subsection{Results analysis}

\subsection{Select the best model}


% Section 7: Apply model
\section{Apply the model}


% Section 8: Post-analysis
\section{Post-analysis}
\subsection{Advantages/disadvantages of the approaches}
Some TODO

Advantages:

Disadvantages:
\subsection{Potential improvement}
To make the application into commercial product, 

\subsection{Other models/approaches}
In this project, 

% Reference
\begin{thebibliography}{8}
\bibitem{ref_red_list}
IUCN 2021, The IUCN Red List of Threatened Species, IUCN Red List of Threatened Species, IUCN.

\bibitem{ref_google_image}
Google Images n.d., Google.com.

\bibitem{ref_roboflow}
Roboflow: Go from Raw Images to a Trained Computer Vision Model in Minutes. n.d., roboflow.ai.

\bibitem{resize}
Saponara, S \& Elhanashi, A 2022, `Impact of Image Resizing on Deep Learning Detectors for Training Time and Model Performance', Lecture Notes in Electrical Engineering, pp. 10â€“17.

\bibitem{R-CNN}
Girshick, R, Donahue, J, Darrell, T \& Malik, J n.d., Rich feature hierarchies for accurate object detection and semantic segmentation Tech report (v5).

\bibitem{Faster R-CNN}
Ren, S, He, K, Girshick, R \& Sun, J 2015, Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, arXiv.org.

\bibitem{YOLO}
Redmon, J, Divvala, S, Girshick, R \& Farhadi, A 2015, You Only Look Once: Unified, Real-Time Object Detection, arXiv.org.



\end{thebibliography}
\end{document}
